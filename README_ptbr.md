# BEES Data Engineering ‚Äì Breweries Case

## üéØ Objetivo

Este projeto tem como objetivo demonstrar a implementa√ß√£o de uma pipeline de dados baseada na arquitetura Medallion (Bronze, Silver, Gold), utilizando PySpark, Apache Airflow e armazenando os dados no MinIO. Os dados s√£o provenientes da API [Open Brewery DB](https://www.openbrewerydb.org/).

---

## üß± Arquitetura da pipeline

###  Bronze 
- Respons√°vel pela inges√£o bruta dos dados da API.
- Os dados s√£o persistidos em formato JSON com timestamp no nome do arquivo.
- Estrutura: `s3a://bronze/breweries_YYYY-MM-DD_HH-MM-SS.json`

### Silver 
- Camada de transforma√ß√£o e limpeza:
  - Normaliza√ß√£o de caracteres (acentos removidos).
  - Convers√£o para formato columnar Parquet.
  - Particionamento por `country` e `state` para facilitar o consumo posterior.
  - Cria√ß√£o da coluna `active_status` para facilitar filtros posteriores.
  - Drop de colunas como `phone` e `website_url` por n√£o serem relevantes para as agrega√ß√µes solicitadas.
  - Concatena√ß√£o de `address_1`, `address_2`, `address_3` em uma √∫nica coluna `address`.
  - Aplica√ß√£o de um schema expl√≠cito para leitura dos dados brutos.

### Gold 
- Camada de agrega√ß√µes anal√≠ticas. Atualmente foram geradas:
  - Vis√£o principal: quantidade de cervejarias por tipo e pa√≠s.
  - Vis√£o extra: Top 10 estados com mais cervejarias ativas.
- Os dados continuam sendo salvos em Parquet, mas n√£o s√£o mais particionados.

> A camada Gold pode ter desdobramentos diferentes conforme o consumo: conex√£o com ferramentas de BI, valida√ß√£o de dados em DuckDB, ou envio para bancos (como Redshift, BigQuery, etc). 

---

## üóìÔ∏è Orquestra√ß√£o com Airflow

Foram criadas DAGs individuais para cada etapa:
- `bronze_breweries_ingestion`
- `silver_breweries_transformation`
- `gold_breweries_aggregations`
- E uma DAG extra, `orchestration_dag`, que orquestra toda a pipeline de ponta a ponta (Bronze -> Silver -> Gold).

Cada DAG est√° configurada com:
- `retries`, `retry_delay`, `email_on_failure` esta setado com um email, porem nao foi finalizada a sua configuracao.

- Planejamento com `schedule_interval` para execu√ß√£o encadeada ao longo da madrugada.

---

## üõ°Ô∏è Monitoramento e Confiabilidade

- Logs foram inclu√≠dos em todas as camadas para rastreamento.
- O tratamento de falhas foi delegado ao Airflow, que captura os c√≥digos de erro dos scripts via `BashOperator`. Assim, falhas nas etapas da pipeline s√£o automaticamente identificadas e marcadas como `failed`, interrompendo a DAG e permitindo reexecu√ß√£o ou alertas conforme necess√°rio.
- Sobre **monitoramento e alertas**, um caminho vi√°vel seria:
  - Adicionar valida√ß√µes de qualidade de dados (nulos, dom√≠nios v√°lidos, valores fora de padr√£o).
  - Integrar o Airflow com sistemas de notifica√ß√£o como Slack ou e-mail via `on_failure_callback`.

---

## üß™ Testes

Testes unit√°rios foram adicionados para a fun√ß√£o `normalize_ascii`, utilizando `pytest` e `SparkSession local` com m√∫ltiplos casos de entrada, cobrindo cen√°rios simples e com caracteres especiais, porem com um melhor entendimento dos dados com certeza novos testes podem surgir.

---

## üö¢ Cont√™ineres e Ambiente

- Projeto totalmente containerizado com Docker Compose.
- Servi√ßos:
  - Apache Airflow
  - Apache Spark
  - MinIO

> üîπ O MinIO pode ser substitu√≠do diretamente pela AWS S3, j√° que a comunica√ß√£o √© feita via `s3a://` com autentica√ß√£o. Basta criar os buckets `bronze`, `silver` e `gold` no provedor correspondente.

---

## ‚úâÔ∏è Execu√ß√£o local

1. Clone o reposit√≥rio

2. Construa os containers com:
```bash
docker compose build
```

3. Suba os servi√ßos:

```bash
docker compose up
```
4. Acesse o MinIO: [http://localhost:9001](http://localhost:9001)  
   - Usu√°rio: `admin`  
   - Senha: `admin123`

5. **Crie os buckets manualmente** no painel do MinIO:
   - `bronze`
   - `silver`
   - `gold`

6. Acesse o Airflow: [http://localhost:8080](http://localhost:8080)  
   - Usu√°rio: `airflow`  
   - Senha: `airflow`

7. Ative as DAGs desejadas manualmente atrav√©s da interface do Airflow, no menu principal.

Execute as dags na ordem da arquitetura Medallion.
   Ou, alternativamente, execute a `orchestration_dag` caso deseje rodar toda a pipeline de forma encadeada.

8. Acompanhe os logs de cada tarefa no Airflow para verificar o sucesso da execu√ß√£o.

